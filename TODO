TODO

- [x] Add timeout in DB query
- [x] strip trailing / in Base URL arg before init new crawler, add leading / if not present in marked Urls
- [x] Add index to DB
- [x] modify db connection settings: max/min connections, etc
- [x] There might be a case when the marked url is different from url fetched from db, and db entry isMonitored is set to true, decide what2do -> saving both if same baseURL
- [x] extended href validation required, way to ignore urls -> list of strings to check if present, ignore
- [x] add flag to blacklist/whitelist urls based on 1) Content-Type, Content-Length header and 2) file extensions -> handled by ignore list
- [ ] make marked-url mandatory? no; '*' as wildcard to fetch all pages?
- [x] maintain a ignored hash map
- [x] graceful exit v0.5.0
- [x] write monitored urls to files v0.6.0, in: date, path, out
- [x] parse website robots.txt, add disallow paths to ignore list v0.6.0
- [x] take user agent from cmd args v0.6.0
- [x] implement models for SQLite v0.7.0 // find citext alternative: name TEXT COLLATE NOCASE -> not required, sqlite is case-insensitive
- [x] consolidate the WAL file upon exit v0.7.0
- [ ] scrolling logs? v0.8.0
- [ ] auto migrate using go-migrate/query or through query v0.8.0
- [ ] write the web UI to view and manage 'URLs' and 'Pages' models v0.9.0
